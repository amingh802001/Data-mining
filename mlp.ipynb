{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "_GwIJaXpL4SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "MNmUJ7epL3Fj"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "X_test = test.iloc[:, :]\n",
        "X_test = X_test / 255.0\n",
        "X_test = X_test.to_numpy()"
      ],
      "metadata": {
        "id": "obpRjZPbc_Mn"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = pd.read_csv('train.csv')\n",
        "# Get the data and labels\n",
        "X = mnist.iloc[:, 1:]  # Select all columns except the last one (features)\n",
        "y = mnist.iloc[:, 0]   # Select the last column (label)"
      ],
      "metadata": {
        "id": "ll3R7IhLKhPU"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1STIOh9NNWFK",
        "outputId": "08d2bc45-56ee-4412-ece1-630555014582"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42000"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values to [0, 1]\n",
        "X = X / 255.0\n",
        "X = X.to_numpy()"
      ],
      "metadata": {
        "id": "GTqJ8IhlKmhI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=1000, random_state=42)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=1000, random_state=42)"
      ],
      "metadata": {
        "id": "0wjZNA7XKsPi"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the labels\n",
        "def one_hot_encode(y, num_classes=10):\n",
        "    one_hot = np.zeros((y.size, num_classes))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return one_hot\n",
        "\n",
        "y_train_encoded = one_hot_encode(y_train)\n",
        "y_valid_encoded = one_hot_encode(y_valid)\n",
        "y_test_encoded = one_hot_encode(y_test)"
      ],
      "metadata": {
        "id": "PiKRnW11K1Ol"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size = 10"
      ],
      "metadata": {
        "id": "nWgjrG98K6cL"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "s-ZJ34fOQpm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, layer_sizes, activation='relu', task='classification', dropout_rate=0.0):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.task = task\n",
        "        self.activation = activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.dropout_masks = []\n",
        "\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i]))\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
        "\n",
        "    def activation_function(self, Z, derivative=False):\n",
        "        if self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return Z > 0\n",
        "            return np.maximum(0, Z)\n",
        "        elif self.activation == 'tanh':\n",
        "            if derivative:\n",
        "                return 1 - np.tanh(Z) ** 2\n",
        "            return np.tanh(Z)\n",
        "        elif self.activation == 'gelu':\n",
        "            if derivative:\n",
        "                return 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (Z + 0.044715 * Z ** 3)))\n",
        "            return 0.5 * Z * (1 + np.tanh(np.sqrt(2 / np.pi) * (Z + 0.044715 * Z ** 3)))\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X, is_training=True):\n",
        "        A = X\n",
        "        cache = [(None, X)]  # (Z, A) pairs\n",
        "        self.dropout_masks = []\n",
        "\n",
        "        for i, (W, b) in enumerate(zip(self.weights[:-1], self.biases[:-1])):\n",
        "            Z = np.dot(A, W) + b\n",
        "            A = self.activation_function(Z)\n",
        "\n",
        "            # Apply dropout during training\n",
        "            if is_training and self.dropout_rate > 0:\n",
        "                dropout_mask = np.random.rand(*A.shape) > self.dropout_rate\n",
        "                A *= dropout_mask  # Drop neurons\n",
        "                A /= (1 - self.dropout_rate)  # Scale output during training\n",
        "                self.dropout_masks.append(dropout_mask)\n",
        "            else:\n",
        "                self.dropout_masks.append(None)\n",
        "\n",
        "            cache.append((Z, A))\n",
        "\n",
        "        # Output\n",
        "        Z = np.dot(A, self.weights[-1]) + self.biases[-1]\n",
        "        if self.task == 'classification':\n",
        "            A = self.softmax(Z)\n",
        "        elif self.task == 'regression':\n",
        "            A = Z\n",
        "        cache.append((Z, A))\n",
        "\n",
        "        return A, cache\n",
        "\n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        if self.task == 'classification':\n",
        "            return -np.sum(y_true * np.log(y_pred + 1e-8)) / len(y_true)\n",
        "        elif self.task == 'regression':\n",
        "            return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def backward(self, X, y_true, cache):\n",
        "        grads = []\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        Z_last, A_last = cache[-1]\n",
        "        if self.task == 'classification':\n",
        "            dZ = A_last - y_true\n",
        "        elif self.task == 'regression':\n",
        "            dZ = (A_last - y_true) / m\n",
        "\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            A_prev = cache[i][1]\n",
        "            dW = np.dot(A_prev.T, dZ) / m\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "            if i > 0:\n",
        "                Z_prev = cache[i][0]\n",
        "                dA_prev = np.dot(dZ, self.weights[i].T)\n",
        "\n",
        "                # Apply dropout mask\n",
        "                if self.dropout_masks[i - 1] is not None:\n",
        "                    dA_prev *= self.dropout_masks[i - 1]\n",
        "                    dA_prev /= (1 - self.dropout_rate)\n",
        "\n",
        "                dZ = dA_prev * self.activation_function(Z_prev, derivative=True)\n",
        "\n",
        "            grads.append((dW, db))\n",
        "\n",
        "        grads.reverse()\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads, learning_rate):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= learning_rate * grads[i][0]\n",
        "            self.biases[i] -= learning_rate * grads[i][1]\n",
        "\n",
        "    def accuracy(self, y_pred, y_true):\n",
        "        preds = np.argmax(y_pred, axis=1)\n",
        "        true_vals = np.argmax(y_true, axis=1)\n",
        "        return np.mean(preds == true_vals)\n",
        "\n",
        "    def train(self, X_train, y_train, X_valid, y_valid, X_test, epochs, learning_rate, batch_size):\n",
        "        n_samples = X_train.shape[0]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            for batch_start in range(0, n_samples, batch_size):\n",
        "                batch_X = X_train[batch_start:batch_start + batch_size]\n",
        "                batch_y = y_train[batch_start:batch_start + batch_size]\n",
        "\n",
        "\n",
        "                y_pred, cache = self.forward(batch_X)\n",
        "\n",
        "                loss = self.compute_loss(y_pred, batch_y)\n",
        "\n",
        "                grads = self.backward(batch_X, batch_y, cache)\n",
        "\n",
        "                self.update_parameters(grads, learning_rate)\n",
        "\n",
        "\n",
        "            y_pred_valid, _ = self.forward(X_valid, is_training=False)\n",
        "            val_acc = self.accuracy(y_pred_valid, y_valid)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}, Validation Accuracy: {val_acc}\")\n",
        "\n",
        "        y_pred_test, _ = self.forward(X_test, is_training=False)\n",
        "        return y_pred_test\n"
      ],
      "metadata": {
        "id": "wOTJCQw-SH8y"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP(layer_sizes=[784, 32, 10], activation='tanh', task='classification', dropout_rate=0.2)\n",
        "y_pred = mlp.train(X_train, y_train_encoded, X_valid, y_valid_encoded, X_test, epochs=22, learning_rate=0.1, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1rIfu7aYqrF",
        "outputId": "aab9bac1-9965-40a8-f2aa-749d42280f49"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/22, Loss: 0.33723219879319155, Validation Accuracy: 0.905\n",
            "Epoch 2/22, Loss: 0.3961536948096187, Validation Accuracy: 0.921\n",
            "Epoch 3/22, Loss: 0.30210481907975184, Validation Accuracy: 0.932\n",
            "Epoch 4/22, Loss: 0.1688687412064978, Validation Accuracy: 0.934\n",
            "Epoch 5/22, Loss: 0.3648909776964, Validation Accuracy: 0.938\n",
            "Epoch 6/22, Loss: 0.16736143185200103, Validation Accuracy: 0.942\n",
            "Epoch 7/22, Loss: 0.25919729624258886, Validation Accuracy: 0.939\n",
            "Epoch 8/22, Loss: 0.22591423251084375, Validation Accuracy: 0.946\n",
            "Epoch 9/22, Loss: 0.18180360438538432, Validation Accuracy: 0.946\n",
            "Epoch 10/22, Loss: 0.07646227432718081, Validation Accuracy: 0.948\n",
            "Epoch 11/22, Loss: 0.18615574007519495, Validation Accuracy: 0.945\n",
            "Epoch 12/22, Loss: 0.1483283373993225, Validation Accuracy: 0.953\n",
            "Epoch 13/22, Loss: 0.21906376372298153, Validation Accuracy: 0.951\n",
            "Epoch 14/22, Loss: 0.15054406561205266, Validation Accuracy: 0.949\n",
            "Epoch 15/22, Loss: 0.29852721445283004, Validation Accuracy: 0.953\n",
            "Epoch 16/22, Loss: 0.17300550857637662, Validation Accuracy: 0.95\n",
            "Epoch 17/22, Loss: 0.07715364192745099, Validation Accuracy: 0.952\n",
            "Epoch 18/22, Loss: 0.08935756388345482, Validation Accuracy: 0.952\n",
            "Epoch 19/22, Loss: 0.15107376934360653, Validation Accuracy: 0.956\n",
            "Epoch 20/22, Loss: 0.10803783913793683, Validation Accuracy: 0.954\n",
            "Epoch 21/22, Loss: 0.15655491915423592, Validation Accuracy: 0.951\n",
            "Epoch 22/22, Loss: 0.12228140838191355, Validation Accuracy: 0.955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjnO6oTHbg1j",
        "outputId": "a5eada60-da69-4aa8-f442-8bd8a2db49e5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.56060047e-05, 8.05235179e-03, 2.02909123e-02, ...,\n",
              "        3.01541103e-05, 9.69686922e-01, 3.39075597e-05],\n",
              "       [2.27263483e-08, 9.99369073e-01, 6.11265379e-05, ...,\n",
              "        3.29471808e-04, 1.42739355e-04, 7.09888160e-06],\n",
              "       [1.94068952e-06, 2.28616606e-06, 1.31605548e-07, ...,\n",
              "        3.93647453e-03, 1.14755411e-04, 9.90771418e-01],\n",
              "       ...,\n",
              "       [4.04274100e-07, 9.90849325e-01, 7.98583429e-05, ...,\n",
              "        8.11795533e-03, 3.53544262e-04, 1.32079751e-04],\n",
              "       [6.08746499e-05, 3.20128536e-08, 9.99756262e-01, ...,\n",
              "        3.09836051e-06, 4.10161298e-05, 6.87746522e-07],\n",
              "       [7.07087212e-06, 6.31396636e-07, 1.82971285e-04, ...,\n",
              "        3.67625211e-07, 1.27752594e-06, 2.37155433e-07]])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(y_pred, axis=1)\n",
        "pred = list(pred)"
      ],
      "metadata": {
        "id": "znBJqWuyf6a3"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "idx = [i+1 for i in range(len(pred))]\n",
        "\n",
        "df =pd.DataFrame({'ImageId': idx,\n",
        "     'Label': pred})\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "28sWRXj2ggIz"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTx1RLf7igV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}